{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Core.Utility.EpsilonGreedyStrategy import EpsilonGreedyStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'episode 459800'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 56\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(stateHasChanged):\n\u001b[1;32m---> 56\u001b[0m         currentState \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetState\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         action \u001b[38;5;241m=\u001b[39m epsilonGreedy\u001b[38;5;241m.\u001b[39mchooseAction(currentState, policy, episode)\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# action = policy.getRandomAction()\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;66;03m# action = policy.getMaxAction(currentState)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# action = ucb.chooseAction(stateDict[currentState])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ICT SW UNIT\\MachineLearningCode\\ML\\Agent.py:38\u001b[0m, in \u001b[0;36mAgent.getState\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     36\u001b[0m cartPosition, cartVelocity, poleAngle, poleAngleVelocity \u001b[38;5;241m=\u001b[39m observation\n\u001b[0;32m     37\u001b[0m cartPosition \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(cartPosition, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcartPositionSpace[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 38\u001b[0m cartVelocity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcartVelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcartVelocitySpace\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m poleAngle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(poleAngle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoleAngleSpace[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m poleAngleVelocity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(poleAngleVelocity, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoleAngleVelocitySpace[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ICT SW UNIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:5733\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ICT SW UNIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ICT SW UNIT\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from Agent import Agent\n",
    "from Policy import Policy\n",
    "from EpsilonGreedyStrategy import EpsilonGreedyStrategy\n",
    "from EnvironmentManager import EnvironmentManager\n",
    "from IPython.display import clear_output, display\n",
    "from itertools import count\n",
    "from Utility import Utility\n",
    "from DecayingLearningRate import DecayingLearningRate\n",
    "from UCB import UCB \n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.001\n",
    "\n",
    "initial_lr = 0.01\n",
    "decay_rate = 0.000002\n",
    "min_lr = 0.001\n",
    "\n",
    "# epsilon_start = 1\n",
    "# epsilon_end = 0.01\n",
    "# epsilon_decay = 0.001\n",
    "\n",
    "# initial_lr = 0.01\n",
    "# decay_rate = 0.00002\n",
    "# min_lr = 0.001\n",
    "\n",
    "\n",
    "agent = Agent()\n",
    "stateSpace = agent.initializeState()\n",
    "epsilonGreedy = EpsilonGreedyStrategy(epsilon_start, epsilon_end, epsilon_decay)\n",
    "decayingLearningRate = DecayingLearningRate(initial_lr, decay_rate, min_lr)\n",
    "environment = EnvironmentManager()\n",
    "policy = Policy() \n",
    "policy.initializeQValues(stateSpace, environment.actionSpaceSize)\n",
    "ucb = UCB(environment.actionSpaceSize, len(stateSpace), .1)\n",
    "utility = Utility()\n",
    "\n",
    "import numpy as np\n",
    "def getMovingAverage2(period:int, values:list):\n",
    "    t = len(values)\n",
    "    return np.mean(values[max(0, t-period):(t + 1)])\n",
    "\n",
    "numberOfEpisodes = 500000\n",
    "score = [0]\n",
    "scoreCounter = 0\n",
    "stateDict = dict(zip(stateSpace, range(len(stateSpace))))\n",
    "for episode in range(numberOfEpisodes):\n",
    "    stateHasChanged = True\n",
    "        \n",
    "    for step in count():\n",
    "        \n",
    "        \n",
    "      \n",
    "        if(stateHasChanged):\n",
    "            \n",
    "            currentState = agent.getState(environment.observation)\n",
    "            action = epsilonGreedy.chooseAction(currentState, policy, episode)\n",
    "            # action = policy.getRandomAction()\n",
    "            # action = policy.getMaxAction(currentState)\n",
    "            # action = ucb.chooseAction(stateDict[currentState])\n",
    "            reward, terminated = environment.step(action)\n",
    "            # ucb.update(stateDict[currentState], action, reward)\n",
    "            nextState = agent.getState(environment.observation)\n",
    "         \n",
    "           \n",
    "            # policy.setTrainingRate(decayingLearningRate.getLearningRate(episode))\n",
    "            if(not terminated):\n",
    "                # policy.setTrainingRate(0.001)\n",
    "                policy.train(currentState, nextState, reward, action)\n",
    "            \n",
    "            stateHasChanged = currentState != nextState\n",
    "            \n",
    "        else:\n",
    "            reward, terminated = environment.step(action)\n",
    "            nextState = agent.getState(environment.observation)\n",
    "            stateHasChanged = currentState != nextState\n",
    "          \n",
    "        \n",
    "        # print('state has changed: ', stateHasChanged)\n",
    "        # print('currentState: ', currentState)\n",
    "        # print('nextStateState: ', nextState)\n",
    "        # print('action ', action)\n",
    "        # print('reward ', reward)\n",
    "        # print('terminated ', terminated)\n",
    "        \n",
    "        scoreCounter += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if terminated:\n",
    "            reward = -10\n",
    "            policy.train(currentState, nextState, reward, action)\n",
    "            \n",
    "            score.append(scoreCounter)\n",
    "            scoreCounter = 0\n",
    "            environment.reset()\n",
    "            \n",
    "            if(episode % 200 == 0):\n",
    "                clear_output(wait=True)\n",
    "                display(f\"episode {episode}\")\n",
    "                display(int(getMovingAverage2(300, score)))\n",
    "                # utility.plot(300 ,score)\n",
    "            \n",
    "            break\n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "environment.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = EnvironmentManager(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfEpisodes = 15\n",
    "\n",
    "for episode in range(numberOfEpisodes):\n",
    "        \n",
    "    for step in count():\n",
    "        clear_output(wait=True)\n",
    "        # display(f\"step {episode}\")\n",
    "        currentState = agent.getState(environment.observation)\n",
    "        \n",
    "        action = policy.getMaxAction(currentState)\n",
    "        # action = policy.getRandomAction()\n",
    "        reward, terminated = environment.step(action)\n",
    "        nextState = agent.getState(environment.observation)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if terminated:\n",
    "            environment.reset()\n",
    "            break\n",
    "            \n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = []\n",
    "for i, j in policy.Q.items():\n",
    "    if(max(j) > 0):\n",
    "        s.append(f'{i} \\t {j}')\n",
    "        \n",
    "print(len(s))\n",
    "\n",
    "s\n",
    "    \n",
    "\n",
    "\n",
    "# import importlib\n",
    "# ag = importlib.import_module(\"Utility\")\n",
    "# ag = importlib.reload(ag)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
